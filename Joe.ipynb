{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T09:14:17.113978Z",
     "start_time": "2019-10-02T09:14:16.213639Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from ast import literal_eval\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams.update({'font.size': 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T09:14:17.344620Z",
     "start_time": "2019-10-02T09:14:17.115384Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('results.csv', index_col=False)\n",
    "\n",
    "del df['Unnamed: 0']\n",
    "\n",
    "a = []\n",
    "\n",
    "df['target'] = df['target'].apply(literal_eval)\n",
    "joe = df['target'].values\n",
    "for v in joe:\n",
    "    a.append(np.array(v))\n",
    "joe = np.array(a).argmax(1)\n",
    "df['target'] = joe\n",
    "\n",
    "df['mean0'] = df['mean0'].astype('float64')\n",
    "df['std0'] = df['std0'].astype('float64')\n",
    "df['mean1'] = df['mean1'].astype('float64')\n",
    "df['std1'] = df['std1'].astype('float64')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T09:14:17.375289Z",
     "start_time": "2019-10-02T09:14:17.361895Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify(x, threshold):\n",
    "    mean = np.array([x['mean0'], x['mean1']])\n",
    "    std = np.array([x['std0'], x['std1']])\n",
    "    if (std[mean.argmax()] <= threshold):\n",
    "        return mean.argmax()\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T09:14:17.390664Z",
     "start_time": "2019-10-02T09:14:17.377272Z"
    }
   },
   "outputs": [],
   "source": [
    "df['std1'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T09:14:25.154398Z",
     "start_time": "2019-10-02T09:14:17.392180Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "thresholds = list(np.arange(0,0.2,0.01))\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for t in tqdm(thresholds):\n",
    "    testdf = df\n",
    "    testdf['test'] = testdf.apply(classify, args=[t], axis=1)\n",
    "    certain = df[df['test']!=-1]\n",
    "#     print(len(certain))\n",
    "    precision = certain['target'] == certain['test']\n",
    "#     print('precision',precision.mean())\n",
    "    recall = df['target'] == df['test']\n",
    "#     print('recall',recall.mean())\n",
    "    precisions.append(precision.mean())\n",
    "    recalls.append(recall.mean())\n",
    "\n",
    "datf = pd.DataFrame({'threshold':thresholds[1:], 'precision':precisions[1:], 'recall':recalls[1:]})\n",
    "sns.lineplot(data=datf, x='threshold', y='precision')\n",
    "sns.lineplot(data=datf, x='threshold', y='recall')\n",
    "plt.title('Precision and Recall with uncertainty threshold')\n",
    "plt.legend(['precision','recall'])\n",
    "# plt.set_xlabel('confidence threshold')\n",
    "# plt.set_ylabel('precision/recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T09:14:25.169897Z",
     "start_time": "2019-10-02T09:14:25.156003Z"
    }
   },
   "outputs": [],
   "source": [
    "datf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T09:14:25.185307Z",
     "start_time": "2019-10-02T09:14:25.171849Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T09:24:52.615623Z",
     "start_time": "2019-10-02T09:24:52.493025Z"
    }
   },
   "outputs": [],
   "source": [
    "print(i)\n",
    "s = df.iloc[i]\n",
    "print(s['text'])\n",
    "if s['target'] == 0:\n",
    "    colors = ['green', 'red']\n",
    "else:\n",
    "    colors = ['red', 'green']\n",
    "\n",
    "x_0 = [0]\n",
    "x_1 = [1]\n",
    "y_0 = [s['mean0']]\n",
    "y_1 = [s['mean1']]\n",
    "err_0 = [s['std0']*2]\n",
    "err_1 = [s['std1']*2]\n",
    "print(err_0, err_1)\n",
    "plt.figure(figsize=(4,8))\n",
    "plt.errorbar(x_0, y_0, yerr=err_0, fmt='o', capsize=8, color=colors[0])\n",
    "plt.errorbar(x_1, y_1, yerr=err_1, fmt='o', capsize=8, color=colors[1])\n",
    "\n",
    "plt.xticks([0, 1], ['negative', 'positive'])\n",
    "plt.ylim([0,1])\n",
    "plt.xlim([-0.5, 1.5])\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T09:14:25.831514Z",
     "start_time": "2019-10-02T09:14:25.309738Z"
    }
   },
   "outputs": [],
   "source": [
    "df['y_pred'] = df.apply(classify, args=[0.075], axis=1)\n",
    "\n",
    "for t in thresholds:\n",
    "    testdf = df\n",
    "    testdf['test'] = testdf.apply(classify, args=[1-t], axis=1)\n",
    "    certain = df[df['test']!=-1]\n",
    "    precision = certain['target'] == certain['test']\n",
    "#     print('precision',precision.mean())\n",
    "    recall = df['target'] == df['test']\n",
    "#     print('recall',recall.mean())\n",
    "    precisions.append(precision.mean())\n",
    "    recalls.append(recall.mean())\n",
    "    \n",
    "plt.plot(thresholds, precisions)\n",
    "plt.plot(thresholds, recalls)\n",
    "plt.title('Precision and Recall with confidence threshold')\n",
    "plt.legend(['precision','recall'])\n",
    "plt.set_xlabel('confidence threshold')\n",
    "plt.set_ylabel('precision/recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y_pred'] = df.apply(classify, args=[1], axis=1)\n",
    "\n",
    "tdf = df[df['y_pred'] != -1]\n",
    "\n",
    "cm = confusion_matrix(tdf['target'].values, tdf['y_pred'].values)\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = ['negative', 'positive'],\n",
    "                  columns = ['predicted negative', 'predicted positive'])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='g', cmap='GnBu')\n",
    "plt.yticks(np.arange(2)+0.5,['actual negative', 'actual positive'], va='center')\n",
    "plt.title('Confusion matrix without uncertainty threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean TF-IDF per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate td-idf\n",
    "\n",
    "X = df['text'].tolist()\n",
    "\n",
    "# vectorizer = TfidfVectorizer()\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean tf-idf per document\n",
    "\n",
    "tf_idf_avg = []\n",
    "for document in X_tfidf:\n",
    "    tf_idf_avg.append(document.mean())\n",
    "\n",
    "print(tf_idf_avg[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the tf-idf of the first n documents\n",
    "\n",
    "n = 50\n",
    "y = tf_idf_avg[:n]\n",
    "x = np.arange(n)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x, y)\n",
    "plt.xlabel('Document')\n",
    "plt.ylabel('tf-idf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the average tf-idf of the correctly and incorrecyly classified reviews\n",
    "\n",
    "# use a high threshold\n",
    "threshold = 0.15\n",
    "\n",
    "sum_correct = 0\n",
    "sum_incorrect = 0\n",
    "\n",
    "tfidf_sum_correct = 0\n",
    "tfidf_sum_incorrect = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    target = row['target']\n",
    "    predicted = classify(row, threshold)\n",
    "    if predicted == target:\n",
    "        tfidf_sum_correct += tf_idf_avg[index]\n",
    "        sum_correct += 1\n",
    "    else:\n",
    "        sum_incorrect += 1\n",
    "        tfidf_sum_incorrect += tf_idf_avg[index]\n",
    "\n",
    "print(\"average of mean correctly classified reviews:\", tfidf_sum_correct/sum_correct)\n",
    "print(\"average of mean incorrectly classified reviews:\", tfidf_sum_incorrect/sum_incorrect)\n",
    "# print(tfidf_sum_correct)\n",
    "# print(tfidf_sum_incorrect)\n",
    "# print('--------')\n",
    "# print(sum_correct)\n",
    "# print(sum_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster the documents based on their tf-idf\n",
    "\n",
    "clusters=2\n",
    "model = KMeans(n_clusters=clusters, max_iter=100)\n",
    "model.fit(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the results in a pandas dataframe\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'cluster': model.labels_\n",
    "})\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(clusters):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(\" %s\" % terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = \"i liked the characters, it was a good movie\"\n",
    "print(\"test 1:\", test_1)\n",
    "test_1 = vectorizer.transform([test_1])\n",
    "predicted_1 = model.predict(test_1)\n",
    "print(\"prediction\", predicted_1)\n",
    "\n",
    "test_2 = \"bad, really bad, i hated it\"\n",
    "print(\"test 2:\", test_2)\n",
    "test_2 = vectorizer.transform([test_2])\n",
    "predicted_2 = model.predict(test_2)\n",
    "print(\"prediction\", predicted_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(X_train))\n",
    "y = []\n",
    "for review in X_train:\n",
    "    y.append(len(review.split()))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x, y)\n",
    "plt.xlabel('document')\n",
    "plt.ylabel('length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "print(\"average length:\", y.mean())\n",
    "print(\"standard deviation:\", y.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment lexicon\n",
    "# !wget https://gist.githubusercontent.com/bastings/d6f99dcb6c82231b94b013031356ba05/raw/f80a0281eba8621b122012c89c8b5e2200b39fd6/sent_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pripo_type():\n",
    "\n",
    "    with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        lex = f.readlines()\n",
    "\n",
    "    lex_pripo = {}\n",
    "    lex_type = {}\n",
    "\n",
    "    for line in lex:\n",
    "        holder = []\n",
    "        for entry in line.split(' '):\n",
    "            holder.append(entry)\n",
    "\n",
    "        tag, word = holder[2].split('=')\n",
    "        tag, lex_pripo[word] = holder[5].split('=')\n",
    "        lex_pripo[word] = lex_pripo[word][0:-1]\n",
    "        tag, lex_type[word] = holder[0].split('=')\n",
    "    return lex_pripo, lex_type\n",
    "\n",
    "lex_pripo, lex_type = get_pripo_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review(review):\n",
    "    score = 0\n",
    "    for word in review.split():\n",
    "        try:\n",
    "            if lex_pripo[word] == 'positive':\n",
    "                score += 1\n",
    "            if lex_pripo[word] == 'negative':\n",
    "                score += -1\n",
    "        except:\n",
    "            pass\n",
    "    value = 0\n",
    "    if score > 3:\n",
    "        value = 1\n",
    "    return value\n",
    "\n",
    "def count_lex(review):\n",
    "    score = 0\n",
    "    for word in review.split():\n",
    "        try:\n",
    "            if lex_pripo[word] == 'positive':\n",
    "                score += 1\n",
    "            if lex_pripo[word] == 'negative':\n",
    "                score += -1\n",
    "        except:\n",
    "            pass\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def norm_dict(my_dict):\n",
    "    factor = 1.0 / sum(my_dict.values())\n",
    "    for i in my_dict:\n",
    "        my_dict[i] = my_dict[i] * factor\n",
    "    return my_dict\n",
    "\n",
    "threshold = 0.02\n",
    "\n",
    "\n",
    "all_reviews = 0\n",
    "symbolic = 0\n",
    "\n",
    "oeps = 0\n",
    "\n",
    "bad = []\n",
    "good = []\n",
    "bad_dict = dict.fromkeys(range(-22, 35), 0)\n",
    "good_dict = dict.fromkeys(range(-22, 35), 0)\n",
    "uncertain_dict = dict.fromkeys(range(-22, 35), 0)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    all_reviews += 1\n",
    "    review = row['text']\n",
    "    \n",
    "    target = row['target']\n",
    "    \n",
    "    std = np.array([row['std0'], row['std1']])\n",
    "    \n",
    "    prediction = np.array([row['mean0'], row['mean1']]).argmax()\n",
    "    \n",
    "    sentiment = classify_review(review)\n",
    "    sent_score = count_lex(review)\n",
    "    \n",
    "    if sentiment == target:\n",
    "        symbolic += 1\n",
    "    \n",
    "    if (sentiment == target) and (prediction != target):\n",
    "        oeps += 1\n",
    "        \n",
    "    if std[prediction] > threshold:\n",
    "        uncertain_dict[sent_score] += 1\n",
    "\n",
    "    if prediction != target:\n",
    "        bad.append(sent_score)\n",
    "        bad_dict[sent_score] += 1\n",
    "    else:\n",
    "        good.append(sent_score)\n",
    "        good_dict[sent_score] += 1\n",
    "        \n",
    "print('accuracy given by symbolic approach:', symbolic/all_reviews)\n",
    "\n",
    "bad_dict = norm_dict(bad_dict)\n",
    "good_dict = norm_dict(good_dict)\n",
    "uncertain_dict = norm_dict(uncertain_dict)\n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "plt.bar(range(len(bad_dict)), list(bad_dict.values()), align='center')\n",
    "plt.xticks(range(len(bad_dict)), list(bad_dict.keys()), rotation='vertical')\n",
    "plt.show()\n",
    "print(bad_dict)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "plt.bar(range(len(good_dict)), list(good_dict.values()), align='center')\n",
    "plt.xticks(range(len(good_dict)), list(good_dict.keys()), rotation='vertical')\n",
    "plt.show()\n",
    "print(good_dict)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "plt.bar(range(len(uncertain_dict)), list(uncertain_dict.values()), align='center')\n",
    "plt.xticks(range(len(uncertain_dict)), list(uncertain_dict.keys()), rotation='vertical')\n",
    "plt.show()\n",
    "print(uncertain_dict)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
