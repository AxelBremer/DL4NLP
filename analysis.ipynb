{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:57:21.932778Z",
     "start_time": "2019-10-06T14:57:19.150713Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.datasets import imdb\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams.update({'font.size': 24})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:57:22.221452Z",
     "start_time": "2019-10-06T14:57:21.934763Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('results.csv', index_col=False)\n",
    "\n",
    "del df['Unnamed: 0']\n",
    "\n",
    "a = []\n",
    "\n",
    "df['target'] = df['target'].apply(literal_eval)\n",
    "joe = df['target'].values\n",
    "for v in joe:\n",
    "    a.append(np.array(v))\n",
    "joe = np.array(a).argmax(1)\n",
    "df['target'] = joe\n",
    "\n",
    "df['mean0'] = df['mean0'].astype('float64')\n",
    "df['std0'] = df['std0'].astype('float64')\n",
    "df['mean1'] = df['mean1'].astype('float64')\n",
    "df['std1'] = df['std1'].astype('float64')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:57:22.228396Z",
     "start_time": "2019-10-06T14:57:22.223435Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify(x, threshold):\n",
    "    '''\n",
    "    This function classifies a given review as the\n",
    "    class with the highest mean, if the std is lower\n",
    "    than the given threshold. If not, it returns -1\n",
    "    '''\n",
    "    mean = np.array([x['mean0'], x['mean1']])\n",
    "    std = np.array([x['std0'], x['std1']])\n",
    "    if (std[mean.argmax()] <= threshold):\n",
    "        return mean.argmax()\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:57:22.250716Z",
     "start_time": "2019-10-06T14:57:22.230875Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Max std')\n",
    "df['std1'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and recall trade off\n",
    "This cell runs the classify function for a range of given thresholds and plots the precision and recall curve given the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:28:59.388820Z",
     "start_time": "2019-10-06T14:28:38.261186Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thresholds = list(np.arange(0,0.2,0.005))\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for t in tqdm(thresholds):\n",
    "    testdf = df\n",
    "    testdf['test'] = testdf.apply(classify, args=[t], axis=1)\n",
    "    certain = df[df['test']!=-1]\n",
    "    precision = certain['target'] == certain['test']\n",
    "    recall = len(certain)/12500\n",
    "    precisions.append(precision.mean())\n",
    "    recalls.append(recall)\n",
    "\n",
    "datf = pd.DataFrame({'threshold':thresholds[1:], 'precision':precisions[1:], 'recall':recalls[1:]})\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "sns.lineplot(data=datf, x='threshold', y='precision')\n",
    "sns.lineplot(data=datf, x='threshold', y='recall')\n",
    "plt.title('Precision and Recall with uncertainty threshold')\n",
    "plt.legend(['precision','recall'])\n",
    "ax.set_xlabel('Uncertainty threshold')\n",
    "ax.set_ylabel('Precision/recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to sort the dataframe on how certain it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:29:10.359358Z",
     "start_time": "2019-10-06T14:29:10.356878Z"
    }
   },
   "outputs": [],
   "source": [
    "sortd = df\n",
    "# df['test'] = df.apply(classify, args=[1], axis=1)\n",
    "# sortd = df.sort_values(['std0'])\n",
    "# sortd = sortd[sortd['target'] != sortd['test']]\n",
    "# sortd = sortd.reset_index(drop=True)\n",
    "# sortd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative analysis\n",
    "The cells below show the results for each specific review in the 'sortd' dataframe. It shows the review and an error bar plot showing the mean and std of the prediction for each class. The green colored class is the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:29:11.639535Z",
     "start_time": "2019-10-06T14:29:11.636559Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:29:12.937072Z",
     "start_time": "2019-10-06T14:29:12.796705Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Index:',i)\n",
    "s = sortd.iloc[i]\n",
    "print('\\nReview:\\n')\n",
    "print(s['text'].strip())\n",
    "if s['target'] == 0:\n",
    "    colors = ['green', 'red']\n",
    "else:\n",
    "    colors = ['red', 'green']\n",
    "\n",
    "x_0 = [0]\n",
    "x_1 = [1]\n",
    "y_0 = [s['mean0']]\n",
    "y_1 = [s['mean1']]\n",
    "err_0 = [s['std0']*2]\n",
    "err_1 = [s['std1']*2]\n",
    "plt.figure(figsize=(4,8))\n",
    "plt.errorbar(x_0, y_0, yerr=err_0, fmt='o', capsize=8, color=colors[0])\n",
    "plt.errorbar(x_1, y_1, yerr=err_1, fmt='o', capsize=8, color=colors[1])\n",
    "\n",
    "plt.xticks([0, 1], ['negative', 'positive'])\n",
    "plt.ylim([0,1])\n",
    "plt.xlim([-0.5, 1.5])\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:30:12.941892Z",
     "start_time": "2019-10-06T14:30:12.172596Z"
    }
   },
   "outputs": [],
   "source": [
    "df['y_pred'] = df.apply(classify, args=[1], axis=1)\n",
    "\n",
    "tdf = df[df['y_pred'] != -1]\n",
    "\n",
    "cm = confusion_matrix(tdf['target'].values, tdf['y_pred'].values)\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = ['negative', 'positive'],\n",
    "                  columns = ['predicted negative', 'predicted positive'])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "sns.heatmap(df_cm, annot=True, fmt='g', cmap='GnBu', ax=ax)\n",
    "plt.yticks(np.arange(2)+0.5,['actual negative', 'actual positive'], va='center')\n",
    "plt.title('Confusion matrix without an uncertainty threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:30:07.571069Z",
     "start_time": "2019-10-06T14:30:06.596429Z"
    }
   },
   "outputs": [],
   "source": [
    "df['y_pred'] = df.apply(classify, args=[0.075], axis=1)\n",
    "\n",
    "tdf = df[df['y_pred'] != -1]\n",
    "\n",
    "cm = confusion_matrix(tdf['target'].values, tdf['y_pred'].values)\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = ['negative', 'positive'],\n",
    "                  columns = ['predicted negative', 'predicted positive'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "sns.heatmap(df_cm, annot=True, fmt='g', cmap='GnBu', ax=ax)\n",
    "plt.yticks(np.arange(2)+0.5,['actual negative', 'actual positive'], va='center')\n",
    "plt.title('Confusion matrix with an uncertainty threshold of 0.075')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive symbolic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment lexicon\n",
    "# !wget https://gist.githubusercontent.com/bastings/d6f99dcb6c82231b94b013031356ba05/raw/f80a0281eba8621b122012c89c8b5e2200b39fd6/sent_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:59:39.621508Z",
     "start_time": "2019-10-06T14:59:39.598692Z"
    }
   },
   "outputs": [],
   "source": [
    "# make a dictionary where {word1: sentiment1, word2: sentiment2}, extracted from the sentiment lexicon\n",
    "\n",
    "def get_pripo_type():\n",
    "\n",
    "    with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        lex = f.readlines()\n",
    "\n",
    "    lex_pripo = {}\n",
    "    lex_type = {}\n",
    "\n",
    "    for line in lex:\n",
    "        holder = []\n",
    "        for entry in line.split(' '):\n",
    "            holder.append(entry)\n",
    "\n",
    "        tag, word = holder[2].split('=')\n",
    "        tag, lex_pripo[word] = holder[5].split('=')\n",
    "        lex_pripo[word] = lex_pripo[word][0:-1]\n",
    "        tag, lex_type[word] = holder[0].split('=')\n",
    "    return lex_pripo, lex_type\n",
    "\n",
    "lex_pripo, _ = get_pripo_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:59:42.235926Z",
     "start_time": "2019-10-06T14:59:42.230470Z"
    }
   },
   "outputs": [],
   "source": [
    "# classify a review with a fixed threshold\n",
    "def classify_review(review):\n",
    "    score = 0\n",
    "    for word in review.split():\n",
    "        try:\n",
    "            if lex_pripo[word] == 'positive':\n",
    "                score += 1\n",
    "            if lex_pripo[word] == 'negative':\n",
    "                score += -1\n",
    "        except:\n",
    "            pass\n",
    "    value = 0\n",
    "    if score > 3:\n",
    "        value = 1\n",
    "    return value\n",
    "\n",
    "# count the sentiment score of a review\n",
    "def count_lex(review):\n",
    "    score = 0\n",
    "    for word in review.split():\n",
    "        try:\n",
    "            if lex_pripo[word] == 'positive':\n",
    "                score += 1\n",
    "            if lex_pripo[word] == 'negative':\n",
    "                score += -1\n",
    "        except:\n",
    "            pass\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:59:46.172185Z",
     "start_time": "2019-10-06T14:59:42.742839Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# normalize the dictionary values\n",
    "def norm_dict(my_dict):\n",
    "    factor = 1.0 / sum(my_dict.values())\n",
    "    for i in my_dict:\n",
    "        my_dict[i] = my_dict[i] * factor\n",
    "    return my_dict\n",
    "\n",
    "# used threshold for model uncertainty\n",
    "threshold = 0.075\n",
    "\n",
    "all_reviews = 0\n",
    "symbolic = 0\n",
    "\n",
    "symbolic_uncertain_correct = []\n",
    "symbolic_uncertain_incorrect = []\n",
    "\n",
    "symbolic_missclass_correct = []\n",
    "symbolic_missclass_incorrect = []\n",
    "\n",
    "bad = []\n",
    "good = []\n",
    "\n",
    "certain = []\n",
    "uncertain = []\n",
    "\n",
    "all_list = []\n",
    "all_dict = dict.fromkeys(range(-22, 35), 0)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    all_reviews += 1\n",
    "\n",
    "    review = row['text']    \n",
    "    target = row['target']\n",
    "    std = np.array([row['std0'], row['std1']])\n",
    "    prediction = np.array([row['mean0'], row['mean1']]).argmax()\n",
    "    \n",
    "    sentiment = classify_review(review)\n",
    "    sent_score = count_lex(review)\n",
    "    \n",
    "    all_dict[sent_score] += 1\n",
    "    all_list.append(sent_score)\n",
    "    \n",
    "    if sentiment == target:\n",
    "        symbolic += 1\n",
    "        \n",
    "    # in the case that our model is uncertain:\n",
    "    if std[prediction] > threshold:\n",
    "        \n",
    "        uncertain.append(sent_score)\n",
    "        \n",
    "        if sentiment == target:\n",
    "            symbolic_uncertain_correct.append(sent_score)\n",
    "        else:\n",
    "            symbolic_uncertain_incorrect.append(sent_score)\n",
    "\n",
    "    # in the case that our model is certain:\n",
    "    else:\n",
    "        certain.append(sent_score)\n",
    "    \n",
    "    # in the case that our model makes a wrong prediction\n",
    "    if prediction != target:\n",
    "        bad.append(sent_score)\n",
    "        \n",
    "        if sentiment == target:\n",
    "            symbolic_missclass_correct.append(sent_score)\n",
    "        else:\n",
    "            symbolic_missclass_incorrect.append(sent_score)\n",
    "\n",
    "    # in the case that our model makes a good prediction\n",
    "    else:\n",
    "        good.append(sent_score)\n",
    "        \n",
    "\n",
    "all_dict = norm_dict(all_dict)\n",
    "\n",
    "# plot the stats\n",
    "\n",
    "print('mean sent score:', sum(all_list)/len(all_list))\n",
    "print('accuracy given by symbolic approach over whole test set:', symbolic/all_reviews)\n",
    "\n",
    "print('-------------------------------')\n",
    "        \n",
    "print(\"when our model was correct we achieve an average sent score of:\", sum(good)/len(good), len(good))\n",
    "print('when our model was incorrect we achieve an average sent score of:', sum(bad)/len(bad), len(bad))\n",
    "a = np.array(good)\n",
    "b = np.array(bad)\n",
    "print('std correct', a.std())\n",
    "print('std incorrect', b.std())\n",
    "\n",
    "print('when our model was certain we achieve an average sent score of:', sum(certain)/len(certain), len(certain))\n",
    "print('when our model was uncertain we achieve an average sent score of:', sum(uncertain)/len(uncertain), len(uncertain))\n",
    "a = np.array(certain)\n",
    "b = np.array(uncertain)\n",
    "print('std certain', a.std())\n",
    "print('std uncertain', b.std())\n",
    "\n",
    "print('-------------------------------')\n",
    "        \n",
    "print('number of correclty classified by naive approach when model was wrong:', len(symbolic_missclass_correct))\n",
    "print('mean sent score:', sum(symbolic_missclass_correct)/len(symbolic_missclass_correct))\n",
    "print('number of incorrectly classified by naive approach when model was wrong', len(symbolic_missclass_incorrect))\n",
    "print('mean sent score', sum(symbolic_missclass_incorrect)/len(symbolic_missclass_incorrect))\n",
    "\n",
    "print('-------------------------------')\n",
    "\n",
    "print('number of correclty classified by naive approach when model was uncertain', len(symbolic_uncertain_correct))\n",
    "print('mean sent_score:', sum(symbolic_uncertain_correct)/len(symbolic_uncertain_correct))\n",
    "print('number of uncorreclty classified by naive approach when model was uncertain', len(symbolic_uncertain_incorrect))\n",
    "print('mean sent_score:', sum(symbolic_uncertain_incorrect)/len(symbolic_uncertain_incorrect))\n",
    "        \n",
    "\n",
    "# plot the distribution\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "plt.bar(range(len(all_dict)), list(all_dict.values()), align='center')\n",
    "\n",
    "x_labels = list(all_dict.keys())\n",
    "x_labels = range(min(x_labels), max(x_labels), 5)\n",
    "\n",
    "plt.xticks(range(0, len(all_dict), 5), list(x_labels), fontsize=16)\n",
    "\n",
    "plt.xlabel('Sentiment score', fontsize=22)\n",
    "plt.ylabel('Percentage of reviews', fontsize=22)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
