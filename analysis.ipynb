{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:57:21.932778Z",
     "start_time": "2019-10-06T14:57:19.150713Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.datasets import imdb\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams.update({'font.size': 24})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:57:22.221452Z",
     "start_time": "2019-10-06T14:57:21.934763Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('results.csv', index_col=False)\n",
    "\n",
    "del df['Unnamed: 0']\n",
    "\n",
    "a = []\n",
    "\n",
    "df['target'] = df['target'].apply(literal_eval)\n",
    "joe = df['target'].values\n",
    "for v in joe:\n",
    "    a.append(np.array(v))\n",
    "joe = np.array(a).argmax(1)\n",
    "df['target'] = joe\n",
    "\n",
    "df['mean0'] = df['mean0'].astype('float64')\n",
    "df['std0'] = df['std0'].astype('float64')\n",
    "df['mean1'] = df['mean1'].astype('float64')\n",
    "df['std1'] = df['std1'].astype('float64')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:57:22.228396Z",
     "start_time": "2019-10-06T14:57:22.223435Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify(x, threshold):\n",
    "    '''\n",
    "    This function classifies a given review as the\n",
    "    class with the highest mean, if the std is lower\n",
    "    than the given threshold. If not, it returns -1\n",
    "    '''\n",
    "    mean = np.array([x['mean0'], x['mean1']])\n",
    "    std = np.array([x['std0'], x['std1']])\n",
    "    if (std[mean.argmax()] <= threshold):\n",
    "        return mean.argmax()\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:57:22.250716Z",
     "start_time": "2019-10-06T14:57:22.230875Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Max std')\n",
    "df['std1'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and recall trade off\n",
    "This cell runs the classify function for a range of given thresholds and plots the precision and recall curve given the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:28:59.388820Z",
     "start_time": "2019-10-06T14:28:38.261186Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thresholds = list(np.arange(0,0.2,0.005))\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for t in tqdm(thresholds):\n",
    "    testdf = df\n",
    "    testdf['test'] = testdf.apply(classify, args=[t], axis=1)\n",
    "    certain = df[df['test']!=-1]\n",
    "    precision = certain['target'] == certain['test']\n",
    "    recall = len(certain)/12500\n",
    "    precisions.append(precision.mean())\n",
    "    recalls.append(recall)\n",
    "\n",
    "datf = pd.DataFrame({'threshold':thresholds[1:], 'precision':precisions[1:], 'recall':recalls[1:]})\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "sns.lineplot(data=datf, x='threshold', y='precision')\n",
    "sns.lineplot(data=datf, x='threshold', y='recall')\n",
    "plt.title('Precision and Recall with uncertainty threshold')\n",
    "plt.legend(['precision','recall'])\n",
    "ax.set_xlabel('Uncertainty threshold')\n",
    "ax.set_ylabel('Precision/recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to sort the dataframe on how certain it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:29:10.359358Z",
     "start_time": "2019-10-06T14:29:10.356878Z"
    }
   },
   "outputs": [],
   "source": [
    "sortd = df\n",
    "# df['test'] = df.apply(classify, args=[1], axis=1)\n",
    "# sortd = df.sort_values(['std0'])\n",
    "# sortd = sortd[sortd['target'] != sortd['test']]\n",
    "# sortd = sortd.reset_index(drop=True)\n",
    "# sortd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative analysis\n",
    "The cells below show the results for each specific review in the 'sortd' dataframe. It shows the review and an error bar plot showing the mean and std of the prediction for each class. The green colored class is the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:29:11.639535Z",
     "start_time": "2019-10-06T14:29:11.636559Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:29:12.937072Z",
     "start_time": "2019-10-06T14:29:12.796705Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Index:',i)\n",
    "s = sortd.iloc[i]\n",
    "print('\\nReview:\\n')\n",
    "print(s['text'].strip())\n",
    "if s['target'] == 0:\n",
    "    colors = ['green', 'red']\n",
    "else:\n",
    "    colors = ['red', 'green']\n",
    "\n",
    "x_0 = [0]\n",
    "x_1 = [1]\n",
    "y_0 = [s['mean0']]\n",
    "y_1 = [s['mean1']]\n",
    "err_0 = [s['std0']*2]\n",
    "err_1 = [s['std1']*2]\n",
    "plt.figure(figsize=(4,8))\n",
    "plt.errorbar(x_0, y_0, yerr=err_0, fmt='o', capsize=8, color=colors[0])\n",
    "plt.errorbar(x_1, y_1, yerr=err_1, fmt='o', capsize=8, color=colors[1])\n",
    "\n",
    "plt.xticks([0, 1], ['negative', 'positive'])\n",
    "plt.ylim([0,1])\n",
    "plt.xlim([-0.5, 1.5])\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:30:12.941892Z",
     "start_time": "2019-10-06T14:30:12.172596Z"
    }
   },
   "outputs": [],
   "source": [
    "df['y_pred'] = df.apply(classify, args=[1], axis=1)\n",
    "\n",
    "tdf = df[df['y_pred'] != -1]\n",
    "\n",
    "cm = confusion_matrix(tdf['target'].values, tdf['y_pred'].values)\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = ['negative', 'positive'],\n",
    "                  columns = ['predicted negative', 'predicted positive'])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "sns.heatmap(df_cm, annot=True, fmt='g', cmap='GnBu', ax=ax)\n",
    "plt.yticks(np.arange(2)+0.5,['actual negative', 'actual positive'], va='center')\n",
    "plt.title('Confusion matrix without an uncertainty threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:30:07.571069Z",
     "start_time": "2019-10-06T14:30:06.596429Z"
    }
   },
   "outputs": [],
   "source": [
    "df['y_pred'] = df.apply(classify, args=[0.075], axis=1)\n",
    "\n",
    "tdf = df[df['y_pred'] != -1]\n",
    "\n",
    "cm = confusion_matrix(tdf['target'].values, tdf['y_pred'].values)\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = ['negative', 'positive'],\n",
    "                  columns = ['predicted negative', 'predicted positive'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "sns.heatmap(df_cm, annot=True, fmt='g', cmap='GnBu', ax=ax)\n",
    "plt.yticks(np.arange(2)+0.5,['actual negative', 'actual positive'], va='center')\n",
    "plt.title('Confusion matrix with an uncertainty threshold of 0.075')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean TF-IDF per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:30:50.460530Z",
     "start_time": "2019-10-06T14:30:48.692785Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate td-idf\n",
    "\n",
    "X = df['text'].tolist()\n",
    "\n",
    "# vectorizer = TfidfVectorizer()\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:30:55.220647Z",
     "start_time": "2019-10-06T14:30:51.228836Z"
    }
   },
   "outputs": [],
   "source": [
    "# compute the mean tf-idf per document\n",
    "\n",
    "tf_idf_avg = []\n",
    "for document in X_tfidf:\n",
    "    tf_idf_avg.append(document.mean())\n",
    "\n",
    "print(tf_idf_avg[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:30:57.656506Z",
     "start_time": "2019-10-06T14:30:57.270121Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the tf-idf of the first n documents\n",
    "\n",
    "n = 50\n",
    "y = tf_idf_avg[:n]\n",
    "x = np.arange(n)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x, y)\n",
    "plt.xlabel('Document')\n",
    "plt.ylabel('tf-idf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:31:05.986337Z",
     "start_time": "2019-10-06T14:31:03.941823Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the average tf-idf of the correctly and incorrecyly classified reviews\n",
    "\n",
    "# use a high threshold\n",
    "threshold = 0.15\n",
    "\n",
    "sum_correct = 0\n",
    "sum_incorrect = 0\n",
    "\n",
    "tfidf_sum_correct = 0\n",
    "tfidf_sum_incorrect = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    target = row['target']\n",
    "    predicted = classify(row, threshold)\n",
    "    if predicted == target:\n",
    "        tfidf_sum_correct += tf_idf_avg[index]\n",
    "        sum_correct += 1\n",
    "    else:\n",
    "        sum_incorrect += 1\n",
    "        tfidf_sum_incorrect += tf_idf_avg[index]\n",
    "\n",
    "print(\"average of mean correctly classified reviews:\", tfidf_sum_correct/sum_correct)\n",
    "print(\"average of mean incorrectly classified reviews:\", tfidf_sum_incorrect/sum_incorrect)\n",
    "# print(tfidf_sum_correct)\n",
    "# print(tfidf_sum_incorrect)\n",
    "# print('--------')\n",
    "# print(sum_correct)\n",
    "# print(sum_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:40:52.253394Z",
     "start_time": "2019-10-06T14:31:28.030084Z"
    }
   },
   "outputs": [],
   "source": [
    "# cluster the documents based on their tf-idf\n",
    "\n",
    "clusters=2\n",
    "model = KMeans(n_clusters=clusters, max_iter=100)\n",
    "model.fit(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:47:15.987829Z",
     "start_time": "2019-10-06T14:47:15.978405Z"
    }
   },
   "outputs": [],
   "source": [
    "# put the results in a pandas dataframe\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'cluster': model.labels_\n",
    "})\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:47:20.787129Z",
     "start_time": "2019-10-06T14:47:20.758857Z"
    }
   },
   "outputs": [],
   "source": [
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(clusters):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(\" %s\" % terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:47:31.665907Z",
     "start_time": "2019-10-06T14:47:31.646563Z"
    }
   },
   "outputs": [],
   "source": [
    "test_1 = \"i liked the characters, it was a good movie\"\n",
    "print(\"test 1:\", test_1)\n",
    "test_1 = vectorizer.transform([test_1])\n",
    "predicted_1 = model.predict(test_1)\n",
    "print(\"prediction\", predicted_1)\n",
    "\n",
    "test_2 = \"bad, really bad, i hated it\"\n",
    "print(\"test 2:\", test_2)\n",
    "test_2 = vectorizer.transform([test_2])\n",
    "predicted_2 = model.predict(test_2)\n",
    "print(\"prediction\", predicted_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:57:32.108227Z",
     "start_time": "2019-10-06T14:57:28.902578Z"
    }
   },
   "outputs": [],
   "source": [
    "# save np.load\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# call load_data with allow_pickle implicitly set to true\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:59:30.816012Z",
     "start_time": "2019-10-06T14:59:30.808572Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(len(x_train))\n",
    "\n",
    "y = []\n",
    "for review in x_train:\n",
    "    y.append(len(review))\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.bar(x, y)\n",
    "# plt.xlabel('document')\n",
    "# plt.ylabel('length')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:59:33.872366Z",
     "start_time": "2019-10-06T14:59:33.861951Z"
    }
   },
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "print(\"average length:\", y.mean())\n",
    "print(\"standard deviation:\", y.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment lexicon\n",
    "# !wget https://gist.githubusercontent.com/bastings/d6f99dcb6c82231b94b013031356ba05/raw/f80a0281eba8621b122012c89c8b5e2200b39fd6/sent_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:59:39.621508Z",
     "start_time": "2019-10-06T14:59:39.598692Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pripo_type():\n",
    "\n",
    "    with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        lex = f.readlines()\n",
    "\n",
    "    lex_pripo = {}\n",
    "    lex_type = {}\n",
    "\n",
    "    for line in lex:\n",
    "        holder = []\n",
    "        for entry in line.split(' '):\n",
    "            holder.append(entry)\n",
    "\n",
    "        tag, word = holder[2].split('=')\n",
    "        tag, lex_pripo[word] = holder[5].split('=')\n",
    "        lex_pripo[word] = lex_pripo[word][0:-1]\n",
    "        tag, lex_type[word] = holder[0].split('=')\n",
    "    return lex_pripo, lex_type\n",
    "\n",
    "lex_pripo, lex_type = get_pripo_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:59:42.235926Z",
     "start_time": "2019-10-06T14:59:42.230470Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_review(review):\n",
    "    score = 0\n",
    "    for word in review.split():\n",
    "        try:\n",
    "            if lex_pripo[word] == 'positive':\n",
    "                score += 1\n",
    "            if lex_pripo[word] == 'negative':\n",
    "                score += -1\n",
    "        except:\n",
    "            pass\n",
    "    value = 0\n",
    "    if score > 3:\n",
    "        value = 1\n",
    "    return value\n",
    "\n",
    "def count_lex(review):\n",
    "    score = 0\n",
    "    for word in review.split():\n",
    "        try:\n",
    "            if lex_pripo[word] == 'positive':\n",
    "                score += 1\n",
    "            if lex_pripo[word] == 'negative':\n",
    "                score += -1\n",
    "        except:\n",
    "            pass\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T14:59:46.172185Z",
     "start_time": "2019-10-06T14:59:42.742839Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def norm_dict(my_dict):\n",
    "    factor = 1.0 / sum(my_dict.values())\n",
    "    for i in my_dict:\n",
    "        my_dict[i] = my_dict[i] * factor\n",
    "    return my_dict\n",
    "\n",
    "threshold = 0.075\n",
    "\n",
    "\n",
    "all_reviews = 0\n",
    "symbolic = 0\n",
    "\n",
    "oeps = 0\n",
    "joe1 = []\n",
    "joe2 = []\n",
    "\n",
    "bad = []\n",
    "good = []\n",
    "bad_dict = dict.fromkeys(range(-22, 35), 0)\n",
    "good_dict = dict.fromkeys(range(-22, 35), 0)\n",
    "uncertain_dict = dict.fromkeys(range(-20, 33), 0)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    all_reviews += 1\n",
    "    review = row['text']\n",
    "    \n",
    "    target = row['target']\n",
    "    \n",
    "    std = np.array([row['std0'], row['std1']])\n",
    "    \n",
    "    prediction = np.array([row['mean0'], row['mean1']]).argmax()\n",
    "    \n",
    "    sentiment = classify_review(review)\n",
    "    sent_score = count_lex(review)\n",
    "    \n",
    "    if sentiment == target:\n",
    "        symbolic += 1\n",
    "    \n",
    "    if (sentiment == target) and (prediction != target):\n",
    "        oeps += 1\n",
    "        \n",
    "    if std[prediction] > threshold:\n",
    "        uncertain_dict[sent_score] += 1\n",
    "        if sentiment == target:\n",
    "            joe1.append(sent_score)\n",
    "        else:\n",
    "            joe2.append(sent_score)\n",
    "\n",
    "    if prediction != target:\n",
    "        bad.append(sent_score)\n",
    "        bad_dict[sent_score] += 1\n",
    "    else:\n",
    "        good.append(sent_score)\n",
    "        good_dict[sent_score] += 1\n",
    "        \n",
    "print('accuracy given by symbolic approach:', symbolic/all_reviews)\n",
    "# print(oeps)\n",
    "print('number of correclty classified by naive approach when model was uncertain', len(joe1))\n",
    "print('mean sent_score:', sum(joe1)/len(joe1))\n",
    "print('number of uncorreclty classified by naive approach when model was uncertain', len(joe2))\n",
    "print('mean sent_score:', sum(joe2)/len(joe2))\n",
    "\n",
    "\n",
    "bad_dict = norm_dict(bad_dict)\n",
    "good_dict = norm_dict(good_dict)\n",
    "uncertain_dict = norm_dict(uncertain_dict)\n",
    "        \n",
    "# fig, ax = plt.subplots(figsize=(15,10))\n",
    "# plt.bar(range(len(bad_dict)), list(bad_dict.values()), align='center')\n",
    "# plt.xticks(range(len(bad_dict)), list(bad_dict.keys()), rotation='vertical')\n",
    "# plt.show()\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(15,10))\n",
    "# plt.bar(range(len(good_dict)), list(good_dict.values()), align='center')\n",
    "# plt.xticks(range(len(good_dict)), list(good_dict.keys()), rotation='vertical')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "plt.bar(range(len(uncertain_dict)), list(uncertain_dict.values()), align='center')\n",
    "\n",
    "x_labels = list(uncertain_dict.keys())\n",
    "x_labels = range(min(x_labels), max(x_labels), 5)\n",
    "\n",
    "# plt.xticks(range(len(uncertain_dict)), list(uncertain_dict.keys()), rotation='vertical')\n",
    "\n",
    "plt.xticks(range(0, len(uncertain_dict), 5), list(x_labels), fontsize=16)\n",
    "\n",
    "# plt.rc('axes', titlesize=10) \n",
    "plt.xlabel('Sentiment score', fontsize=22)\n",
    "plt.ylabel('Percentage of reviews', fontsize=22)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
